{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6ba392e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Motive Help Center RAG (Bedrock + Titan v1)\n",
    "\n",
    "!pip install boto3 pandas numpy scikit-learn --quiet\n",
    "\n",
    "import os, re, json, time, uuid, random, hashlib, pickle\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List, Dict, Any, Tuple, Set, Optional\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import boto3\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edcaa992",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "AWS_REGION = \"us-east-1\"\n",
    "# Prefer environment/instance role for credentials. If needed, set env vars or hardcode (not recommended).\n",
    "AWS_ACCESS_KEY_ID = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "AWS_SECRET_ACCESS_KEY = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "\n",
    "LOCAL_WEB_FILE = \"web_content.txt\"\n",
    "\n",
    "# Models\n",
    "GEN_MODEL_PRIMARY   = \"us.anthropic.claude-3-5-sonnet-20241022-v2:0\"\n",
    "GEN_MODEL_FALLBACK  = \"us.amazon.nova-pro-v1:0\"\n",
    "EMBED_MODEL         = \"amazon.titan-embed-text-v1\"\n",
    "\n",
    "# Chunking & retrieval\n",
    "CHUNK_SIZE     = 2000       # fewer, larger chunks → fewer calls\n",
    "CHUNK_OVERLAP  = 300\n",
    "MIN_PAGE_CHARS = 120\n",
    "SKIP_URL_PATTERNS = (r\"\\?post_type=\", r\"\\?attachment_id=\")\n",
    "\n",
    "# Hybrid retrieval\n",
    "LEXICAL_CANDIDATES = 80    # per query; embed only these\n",
    "TOP_K_FINAL        = 10     # final retrieved for answer\n",
    "ALPHA_HYBRID       = 0.55   # mix of vector + lexical (0..1)\n",
    "\n",
    "# Titan v1 throttle handling\n",
    "EMBED_SLEEP_BASE   = 0.45\n",
    "EMBED_MAX_RETRIES  = 8\n",
    "EMBED_CACHE_PATH   = \"embed_cache_titan_v1.pkl\"\n",
    "\n",
    "# Guardrails\n",
    "SIM_THRESHOLD      = 0.32   # if max sim (vector) under this → ask clarify\n",
    "\n",
    "# Output logs\n",
    "RUN_ID  = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "LOG_JSON = f\"chat_logs_{RUN_ID}.jsonl\"\n",
    "LOG_CSV  = f\"chat_logs_{RUN_ID}.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "010d41eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1) READ + PARSE\n",
    "# ===========================\n",
    "def read_local_text(path: str) -> str:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "@dataclass\n",
    "class Page:\n",
    "    title: str\n",
    "    url: str\n",
    "    content: str\n",
    "\n",
    "def _looks_like_noise(url: str, content: str) -> bool:\n",
    "    if any(re.search(p, url) for p in SKIP_URL_PATTERNS):\n",
    "        return True\n",
    "    if len(content.strip()) < MIN_PAGE_CHARS:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def parse_web_dump(raw: str) -> List[Page]:\n",
    "    blocks = re.split(r\"-{20,}\", raw)\n",
    "    pages: List[Page] = []\n",
    "    for b in blocks:\n",
    "        b = b.strip()\n",
    "        if not b:\n",
    "            continue\n",
    "        title = re.search(r\"^TITLE:\\s*(.+)$\", b, re.M)\n",
    "        url   = re.search(r\"^URL:\\s*(.+)$\", b, re.M)\n",
    "        m_content = re.search(r\"CONTENT:\\s*(.+)\", b, re.S)\n",
    "        m_excerpt = re.search(r\"EXCERPT:\\s*(.+)\", b, re.S)\n",
    "        content = \"\"\n",
    "        if m_content:\n",
    "            content = m_content.group(1).strip()\n",
    "        elif m_excerpt:\n",
    "            content = m_excerpt.group(1).strip()\n",
    "        t = title.group(1).strip() if title else \"Untitled\"\n",
    "        u = url.group(1).strip() if url else \"\"\n",
    "        if _looks_like_noise(u, content):\n",
    "            continue\n",
    "        pages.append(Page(title=t, url=u, content=content))\n",
    "    return pages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ccfd200",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===========================\n",
    "# 2) CHUNKING + DEDUPE\n",
    "# ===========================\n",
    "@dataclass\n",
    "class Chunk:\n",
    "    page_title: str\n",
    "    page_url: str\n",
    "    text: str\n",
    "    chunk_id: str\n",
    "\n",
    "def chunk_text(text: str, size=CHUNK_SIZE, overlap=CHUNK_OVERLAP) -> List[str]:\n",
    "    text = re.sub(r\"\\s+\\n\", \"\\n\", text).strip()\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = min(start + size, len(text))\n",
    "        window = text[start:end]\n",
    "        if end < len(text):\n",
    "            last_stop = max(window.rfind(\". \"), window.rfind(\"\\n\\n\"))\n",
    "            if last_stop > size * 0.6:\n",
    "                end = start + last_stop + 1\n",
    "                window = text[start:end]\n",
    "        window = window.strip()\n",
    "        if window:\n",
    "            chunks.append(window)\n",
    "        start = max(end - overlap, end)\n",
    "    return chunks\n",
    "\n",
    "def build_corpus(pages: List[Page]) -> List[Chunk]:\n",
    "    corpus: List[Chunk] = []\n",
    "    seen = set()\n",
    "    for p in pages:\n",
    "        for txt in chunk_text(p.content):\n",
    "            if len(txt) < 60:\n",
    "                continue\n",
    "            key = re.sub(r\"\\s+\", \" \", txt.strip()).lower()\n",
    "            if key in seen:\n",
    "                continue\n",
    "            seen.add(key)\n",
    "            corpus.append(Chunk(p.title, p.url, txt, chunk_id=str(uuid.uuid4())))\n",
    "    return corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "499f4709",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===========================\n",
    "# 3) TF-IDF LEXICAL INDEX (fast, local)\n",
    "# ===========================\n",
    "class LexicalIndex:\n",
    "    def __init__(self, texts: List[str]):\n",
    "        self.vectorizer = TfidfVectorizer(\n",
    "            ngram_range=(1,2),\n",
    "            max_features=120000,\n",
    "            lowercase=True,\n",
    "            stop_words=\"english\"\n",
    "        )\n",
    "        self.matrix = self.vectorizer.fit_transform(texts)\n",
    "\n",
    "    def topn(self, query: str, n: int) -> List[int]:\n",
    "        qv = self.vectorizer.transform([query])\n",
    "        sims = cosine_similarity(self.matrix, qv).ravel()\n",
    "        idx = np.argsort(-sims)[:n]\n",
    "        return idx.tolist(), sims[idx].astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "aa7429a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===========================\n",
    "# 4) BEDROCK CLIENT + BACKOFF/CACHE\n",
    "# ===========================\n",
    "bedrock_rt = boto3.client(\n",
    "    \"bedrock-runtime\",\n",
    "    region_name=AWS_REGION,\n",
    "    aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
    ")\n",
    "\n",
    "def _load_embed_cache(path: str) -> dict:\n",
    "    if os.path.exists(path):\n",
    "        try:\n",
    "            with open(path, \"rb\") as f:\n",
    "                return pickle.load(f)\n",
    "        except Exception:\n",
    "            return {}\n",
    "    return {}\n",
    "\n",
    "def _save_embed_cache(path: str, cache: dict) -> None:\n",
    "    tmp = path + \".tmp\"\n",
    "    with open(tmp, \"wb\") as f:\n",
    "        pickle.dump(cache, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    os.replace(tmp, path)\n",
    "\n",
    "def _text_hash(t: str) -> str:\n",
    "    return hashlib.sha1(t.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def _invoke_with_retry(model_id: str, body: dict) -> dict:\n",
    "    for attempt in range(EMBED_MAX_RETRIES):\n",
    "        try:\n",
    "            r = bedrock_rt.invoke_model(modelId=model_id, body=json.dumps(body))\n",
    "            return json.loads(r[\"body\"].read())\n",
    "        except Exception as e:\n",
    "            msg = str(e)\n",
    "            if (\"ThrottlingException\" in msg or\n",
    "                \"Too Many Requests\" in msg or\n",
    "                \"ServiceUnavailable\" in msg):\n",
    "                base = EMBED_SLEEP_BASE * (2 ** attempt)\n",
    "                sleep_for = min(base * (0.75 + random.random() * 0.5), 8.0)\n",
    "                time.sleep(sleep_for)\n",
    "                continue\n",
    "            raise\n",
    "    raise RuntimeError(f\"Invoke failed after {EMBED_MAX_RETRIES} retries (model {model_id}).\")\n",
    "\n",
    "def embed_titan_v1(text: str) -> np.ndarray:\n",
    "    out = _invoke_with_retry(EMBED_MODEL, {\"inputText\": text})\n",
    "    return np.array(out[\"embedding\"], dtype=np.float32)\n",
    "\n",
    "def embed_texts_cached(texts: List[str], cache_path=EMBED_CACHE_PATH) -> np.ndarray:\n",
    "    cache = _load_embed_cache(cache_path)\n",
    "    vecs = []\n",
    "    dirty = False\n",
    "    for i, t in enumerate(texts, 1):\n",
    "        h = _text_hash(t)\n",
    "        if h in cache:\n",
    "            vecs.append(cache[h])\n",
    "            continue\n",
    "        v = embed_titan_v1(t)\n",
    "        time.sleep(EMBED_SLEEP_BASE)  # gentle pacing\n",
    "        cache[h] = v\n",
    "        vecs.append(v)\n",
    "        dirty = True\n",
    "        if dirty and (i % 32 == 0):\n",
    "            _save_embed_cache(cache_path, cache)\n",
    "            dirty = False\n",
    "    if dirty:\n",
    "        _save_embed_cache(cache_path, cache)\n",
    "    return np.vstack(vecs)\n",
    "\n",
    "def normalize_rows(m: np.ndarray) -> np.ndarray:\n",
    "    n = np.linalg.norm(m, axis=1, keepdims=True) + 1e-10\n",
    "    return m / n\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "83f96113",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_llm(messages: List[Dict[str, str]], max_tokens: int = 120) -> str:\n",
    "    \"\"\"\n",
    "    Robust Bedrock generation with adjustable max_tokens.\n",
    "    Guarantees at least one user message; retries/backoffs via _invoke_with_retry;\n",
    "    falls back to Nova if Anthropic fails.\n",
    "    \"\"\"\n",
    "    sys_prompt = \"\"\n",
    "    user_turns = []\n",
    "    for m in messages:\n",
    "        if m.get(\"role\") == \"system\":\n",
    "            sys_prompt = m.get(\"content\", \"\") or sys_prompt\n",
    "        else:\n",
    "            if m.get(\"content\", \"\").strip():\n",
    "                user_turns.append({\"role\": m[\"role\"], \"content\": m[\"content\"]})\n",
    "    if not user_turns:\n",
    "        user_turns = [{\"role\": \"user\", \"content\": \"Okay.\"}]\n",
    "\n",
    "    def _anthropic_call() -> str:\n",
    "        anthro_msgs = [\n",
    "            {\"role\": m[\"role\"], \"content\": [{\"type\": \"text\", \"text\": m[\"content\"]}]}\n",
    "            for m in user_turns\n",
    "        ]\n",
    "        body = {\n",
    "            \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "            \"max_tokens\": max_tokens,         # <-- tighter per intent\n",
    "            \"temperature\": 0.2,\n",
    "            \"system\": sys_prompt or \"\",\n",
    "            \"messages\": anthro_msgs\n",
    "        }\n",
    "        out = _invoke_with_retry(GEN_MODEL_PRIMARY, body)\n",
    "        return (out.get(\"content\", [{}])[0].get(\"text\", \"\") or \"\").strip()\n",
    "\n",
    "    def _nova_call() -> str:\n",
    "        transcript = \"\\n\\n\".join([f\"{m['role'].upper()}: {m['content']}\" for m in user_turns])\n",
    "        body = {\n",
    "            \"inputText\": transcript,\n",
    "            \"textGenerationConfig\": {\"temperature\": 0.2, \"maxTokenCount\": max_tokens}\n",
    "        }\n",
    "        out = _invoke_with_retry(GEN_MODEL_FALLBACK, body)\n",
    "        return (out.get(\"outputText\", \"\") or \"\").strip()\n",
    "\n",
    "    try:\n",
    "        txt = _anthropic_call()\n",
    "        return txt if txt else _nova_call()\n",
    "    except Exception:\n",
    "        return _nova_call()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d7381d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# 6) MULTI-TURN + ANSWERING\n",
    "# ===========================\n",
    "CONDENSE_SYSTEM = (\n",
    "    \"You are a helpful support agent assistant. \"\n",
    "    \"Rewrite the user's last message as a standalone, specific query using prior chat turns for context. \"\n",
    "    \"Do not answer; only return the rewritten query.\"\n",
    ")\n",
    "\n",
    "# ---------- INTENT DETECTION ----------\n",
    "import re\n",
    "\n",
    "BOOL_PREFIXES = (\n",
    "    r\"^\\s*does\\b\", r\"^\\s*do\\b\", r\"^\\s*is\\b\", r\"^\\s*are\\b\",\n",
    "    r\"^\\s*can\\b\", r\"^\\s*could\\b\", r\"^\\s*should\\b\", r\"^\\s*will\\b\",\n",
    "    r\"^\\s*did\\b\", r\"^\\s*was\\b\", r\"^\\s*were\\b\",\n",
    ")\n",
    "EXPAND_CUES = (\n",
    "    r\"\\btell me more\\b\", r\"\\bmore details\\b\", r\"\\bwhat else\\b\",\n",
    "    r\"\\belaborate\\b\", r\"\\bcan you expand\\b\", r\"\\blearn more\\b\"\n",
    ")\n",
    "_bool_prefix_re = re.compile(\"|\".join(BOOL_PREFIXES), re.I)\n",
    "_expand_re = re.compile(\"|\".join(EXPAND_CUES), re.I)\n",
    "\n",
    "def detect_intent(user_q: str, is_followup: bool) -> str:\n",
    "    \"\"\"\n",
    "    Returns one of: 'yesno', 'expand', 'definition', 'fact'\n",
    "    - yesno: starts with Does/Is/Can/Are... → 'Yes,'/'No,' lead\n",
    "    - expand: explicit cue OR follow-up asking for more → 2 short sentences\n",
    "    - definition: inherited/classified definition intent (set elsewhere)\n",
    "    - fact: default one concise sentence\n",
    "    \"\"\"\n",
    "    q = user_q.strip()\n",
    "    if _bool_prefix_re.search(q):\n",
    "        return \"yesno\"\n",
    "    if _expand_re.search(q) or (is_followup and re.search(r\"\\b(more|details|what about)\\b\", q, re.I)):\n",
    "        return \"expand\"\n",
    "    return \"fact\"\n",
    "\n",
    "# ---------- ANSWER STYLE PROMPTS ----------\n",
    "ANSWER_SYSTEM_BASE = (\n",
    "    \"You are Motive’s Help Center support agent.\\n\"\n",
    "    \"No URLs, citations, or references. Do not mention 'context' or how you derived the answer.\\n\"\n",
    "    \"Tone: supportive, customer‑service oriented, concise, and factual.\\n\"\n",
    "    \"Avoid phrases like 'according to' or 'as per'.\"\n",
    ")\n",
    "ANSWER_STYLE = {\n",
    "    \"fact\": (\n",
    "        ANSWER_SYSTEM_BASE + \"\\n\"\n",
    "        \"Respond in exactly ONE concise sentence (≤ 20 words). \"\n",
    "        \"Be direct and professional; do not add extra clauses or background.\"\n",
    "    ),\n",
    "    \"yesno\": (\n",
    "        ANSWER_SYSTEM_BASE + \"\\n\"\n",
    "        \"Respond in exactly ONE concise sentence (≤ 20 words). \"\n",
    "        \"Begin with 'Yes,' or 'No,' and state the key detail only.\"\n",
    "    ),\n",
    "    \"expand\": (\n",
    "        ANSWER_SYSTEM_BASE + \"\\n\"\n",
    "        \"Respond in exactly TWO short sentences, each under 15 words. \"\n",
    "        \"Add only one or two concrete specifics present in the context.\"\n",
    "    ),\n",
    "    \"definition\": (\n",
    "        ANSWER_SYSTEM_BASE + \"\\n\"\n",
    "        \"Define the term clearly in ONE concise sentence (≤ 22 words), starting with 'X refers to' or 'X means', then one key criterion.\"\n",
    "    ),\n",
    "}\n",
    "def build_answer_system(intent: str) -> str:\n",
    "    return ANSWER_STYLE.get(intent, ANSWER_STYLE[\"fact\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "75f40f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- sentence-aware fallback, no hard chopping ---\n",
    "_SENTENCE_SPLIT = re.compile(r'(?<!\\bU\\.S)(?<!\\bU)\\.(\\s+|$)|[!?](\\s+|$)')\n",
    "def crop_sentences(text: str, max_sentences: int) -> str:\n",
    "    \"\"\"\n",
    "    Keep up to max_sentences by punctuation, avoiding splits on 'U.S.'.\n",
    "    \"\"\"\n",
    "    s = re.sub(r\"\\s+\", \" \", str(text)).strip()\n",
    "    if not s:\n",
    "        return s\n",
    "    out, count, i = [], 0, 0\n",
    "    for m in _SENTENCE_SPLIT.finditer(s + \" \"):\n",
    "        end = m.start() + 1  # include the punctuation\n",
    "        out.append(s[i:end].strip())\n",
    "        count += 1\n",
    "        i = m.end()\n",
    "        if count >= max_sentences:\n",
    "            break\n",
    "    if count == 0:\n",
    "        return s\n",
    "    clipped = \" \".join(out).strip()\n",
    "    if clipped and clipped[-1] not in \".!?\":\n",
    "        clipped += \".\"\n",
    "    return clipped\n",
    "\n",
    "# ---------- POLISH & GUARDS ----------\n",
    "def polish_one_liner(text: str, max_words: int = 28) -> str:\n",
    "    txt = re.sub(r\"\\s+\", \" \", str(text)).strip()\n",
    "    words = txt.split()\n",
    "    if len(words) > max_words:\n",
    "        txt = \" \".join(words[:max_words])\n",
    "    if not txt.endswith((\".\", \"!\", \"?\")):\n",
    "        txt += \".\"\n",
    "    return txt\n",
    "\n",
    "def polish_two_sentences(text: str, max_words: int = 50) -> str:\n",
    "    txt = re.sub(r\"\\s+\", \" \", str(text)).strip()\n",
    "    words = txt.split()\n",
    "    if len(words) > max_words:\n",
    "        txt = \" \".join(words[:max_words])\n",
    "    count = 0; out = []\n",
    "    for ch in txt:\n",
    "        out.append(ch)\n",
    "        if ch in \".!?\":\n",
    "            count += 1\n",
    "            if count >= 2:\n",
    "                break\n",
    "    cleaned = \"\".join(out).strip()\n",
    "    if cleaned and cleaned[-1] not in \".!?\":\n",
    "        cleaned += \".\"\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c102a8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enforce_grounding_guard(answer: str, context_blocks: str) -> str:\n",
    "    \"\"\"\n",
    "    Two-tier numeric guard:\n",
    "      - If ANY numeric tokens in the answer are present in the retrieved context → keep them.\n",
    "      - If NONE match → rephrase with LLM to remove numbers (keeps it on-topic).\n",
    "    \"\"\"\n",
    "    nums_ans = re.findall(r\"\\b\\d{1,3}(?:[,\\d]{0,12})?(?:\\.\\d+)?%?\\b\", answer)\n",
    "    if not nums_ans:\n",
    "        return answer\n",
    "\n",
    "    ctx = context_blocks or \"\"\n",
    "    for n in nums_ans:\n",
    "        n_plain = n.replace(\",\", \"\")\n",
    "        if (n in ctx) or (n_plain in ctx):\n",
    "            return answer  # keep because at least one number is grounded\n",
    "\n",
    "    # Rephrase fallback without numbers\n",
    "    prompt = (\n",
    "        \"Rewrite the following answer to remove or generalize any numeric claims, \"\n",
    "        \"while keeping it accurate, concise, and in a customer-support tone:\\n\\n\"\n",
    "        f\"Original: {answer}\\n\\nRewritten:\"\n",
    "    )\n",
    "    msgs = [{\"role\": \"system\", \"content\": ANSWER_SYSTEM_BASE},\n",
    "            {\"role\": \"user\", \"content\": prompt}]\n",
    "    try:\n",
    "        return polish_one_liner(generate_llm(msgs, max_tokens=48))\n",
    "    except Exception:\n",
    "        softened = re.sub(r\"\\b\\d{1,3}(?:[,\\d]{0,12})?(?:\\.\\d+)?%?\\b\", \"\", answer)\n",
    "        softened = re.sub(r\"\\s{2,}\", \" \", softened).strip()\n",
    "        if not softened:\n",
    "            softened = \"This feature is designed to improve safety and reliability.\"\n",
    "        if softened[-1] not in \".!?\":\n",
    "            softened += \".\"\n",
    "        return softened\n",
    "\n",
    "def condense_query(history: List[Dict[str, str]]) -> str:\n",
    "    hist = history[-6:] if history else [{\"role\":\"user\",\"content\":\".\"}]\n",
    "    msgs = [{\"role\": \"system\", \"content\": CONDENSE_SYSTEM}] + hist\n",
    "    return generate_llm(msgs, max_tokens=48)\n",
    "\n",
    "def make_context(snippets: List[Tuple[int, float]], corpus: List[Chunk]) -> str:\n",
    "    blocks = []\n",
    "    for idx, sim in snippets[:10]:  # cap to TOP_K_FINAL\n",
    "        ch = corpus[idx]\n",
    "        blocks.append(f\"[{ch.page_title}] (sim={sim:.2f})\\nURL: {ch.page_url}\\n---\\n{ch.text}\\n\")\n",
    "    return \"\\n\\n\".join(blocks)\n",
    "\n",
    "def answer_with_context_dynamic(user_query: str, context_blocks: str, intent: str) -> str:\n",
    "    system_prompt = build_answer_system(intent)\n",
    "    prompt = (\n",
    "        f\"User question:\\n{user_query}\\n\\n\"\n",
    "        f\"Authoritative context snippets:\\n{context_blocks}\\n\\n\"\n",
    "        \"Write the final answer now.\"\n",
    "    )\n",
    "    msgs = [{\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt}]\n",
    "    max_tokens = 80 if intent == \"expand\" else 48\n",
    "    raw = generate_llm(msgs, max_tokens=max_tokens)\n",
    "    raw = enforce_grounding_guard(raw, context_blocks)\n",
    "    if intent == \"expand\":\n",
    "        return crop_sentences(raw, max_sentences=2)\n",
    "    else:\n",
    "        return crop_sentences(raw, max_sentences=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d1a70277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# 7) HYBRID SEARCH (lexical preselect → embed subset → vector sim)\n",
    "# ===========================\n",
    "class HybridSearcher:\n",
    "    def __init__(self, corpus: List[Chunk]):\n",
    "        self.corpus = corpus\n",
    "        self.texts  = [c.text for c in corpus]\n",
    "        self.lex = LexicalIndex(self.texts)\n",
    "\n",
    "    def search(self, query: str, topn_lex=LEXICAL_CANDIDATES, topk_final=TOP_K_FINAL, alpha=ALPHA_HYBRID):\n",
    "        cand_idx, lex_scores = self.lex.topn(query, topn_lex)\n",
    "        cand_texts = [self.texts[i] for i in cand_idx]\n",
    "        Q = normalize_rows(embed_texts_cached([query]))\n",
    "        D = normalize_rows(embed_texts_cached(cand_texts))\n",
    "        vec_scores = (D @ Q.T).ravel()\n",
    "        lex_norm = (lex_scores - np.min(lex_scores)) / (np.max(lex_scores) - np.min(lex_scores) + 1e-9)\n",
    "        hybrid = alpha * vec_scores + (1 - alpha) * lex_norm\n",
    "        order = np.argsort(-hybrid)[:topk_final]\n",
    "        top = [(cand_idx[i], float(vec_scores[i])) for i in order]\n",
    "        return top\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9a97e236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Intent inheritance (semantic, generic) + Concept lexicon\n",
    "# ===========================\n",
    "INTENT_LABELS = [\"definition\", \"capability\", \"comparison\", \"policy\", \"procedure\", \"troubleshooting\", \"benefit\", \"other\"]\n",
    "_INTENT_CACHE: Dict[str, str] = {}\n",
    "CONCEPT_TERMS: Set[str] | None = None  # built lazily from corpus titles\n",
    "\n",
    "def _hash_text(t: str) -> str:\n",
    "    import hashlib\n",
    "    return hashlib.sha1(t.strip().lower().encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def classify_semantic_intent(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Classify question into one of INTENT_LABELS using a tiny LLM prompt. Cached.\n",
    "    \"\"\"\n",
    "    key = _hash_text(text)\n",
    "    if key in _INTENT_CACHE:\n",
    "        return _INTENT_CACHE[key]\n",
    "    sys = (\n",
    "        \"Classify the user question into one label from this set only:\\n\"\n",
    "        \"definition, capability, comparison, policy, procedure, troubleshooting, benefit, other.\\n\"\n",
    "        \"Return ONLY the label.\"\n",
    "    )\n",
    "    prompt = f\"Question: {text}\\nLabel:\"\n",
    "    msgs = [{\"role\":\"system\",\"content\":sys}, {\"role\":\"user\",\"content\":prompt}]\n",
    "    label = generate_llm(msgs, max_tokens=8).strip().lower()\n",
    "    if label not in INTENT_LABELS:\n",
    "        label = \"other\"\n",
    "    _INTENT_CACHE[key] = label\n",
    "    return label\n",
    "\n",
    "def _normalize_term(s: str) -> str:\n",
    "    s = s.strip().lower()\n",
    "    s = re.sub(r\"[^a-z0-9\\s-]\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    s = s.replace(\"-\", \" \")\n",
    "    return s\n",
    "\n",
    "def build_concept_lexicon(corpus: List[Chunk]) -> Set[str]:\n",
    "    \"\"\"\n",
    "    Build a set of short, title-derived concept terms: e.g., 'unsafe parking', 'forward collision warning',\n",
    "    'cellphone usage', 'close following', etc.\n",
    "    \"\"\"\n",
    "    terms: Set[str] = set()\n",
    "    for ch in corpus:\n",
    "        t = _normalize_term(ch.page_title)\n",
    "        if not t:\n",
    "            continue\n",
    "        # keep short titles/phrases\n",
    "        if 1 <= len(t.split()) <= 5:\n",
    "            terms.add(t)\n",
    "        # also add split variants for hyphenated forms in title\n",
    "        if \"-\" in ch.page_title:\n",
    "            terms.add(_normalize_term(ch.page_title.replace(\"-\", \" \")))\n",
    "    return terms\n",
    "\n",
    "def is_fragment_like(q: str, max_tokens: int = 4) -> bool:\n",
    "    s = q.strip().strip(\"?!.\").strip()\n",
    "    if not s:\n",
    "        return False\n",
    "    toks = s.split()\n",
    "    if len(toks) > max_tokens:\n",
    "        return False\n",
    "    if re.match(r\"^(what|who|how|why|when|where|does|do|is|are|can|should|will|did|was|were)\\b\", s, re.I):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def rewrite_fragment_with_intent(intent: str, fragment: str) -> str:\n",
    "    f = fragment.strip().strip(\"?!.\")\n",
    "    if intent == \"definition\":\n",
    "        return f\"What is {f}?\"\n",
    "    elif intent == \"capability\":\n",
    "        return f\"Can it handle {f}?\"\n",
    "    elif intent == \"comparison\":\n",
    "        return f\"How does it compare regarding {f}?\"\n",
    "    elif intent == \"policy\":\n",
    "        return f\"What are the compliance or policy details for {f}?\"\n",
    "    elif intent == \"procedure\":\n",
    "        return f\"How do I configure or use {f}?\"\n",
    "    elif intent == \"troubleshooting\":\n",
    "        return f\"How can I diagnose or resolve issues with {f}?\"\n",
    "    elif intent == \"benefit\":\n",
    "        return f\"What are the benefits related to {f}?\"\n",
    "    else:\n",
    "        return f\"Could you explain {f}?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "044422bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Semantic Follow-up Detection with Topic Memory ======\n",
    "@dataclass\n",
    "class Topic:\n",
    "    topic_id: int\n",
    "    centroid: np.ndarray              # normalized embedding EMA of WITH-history standalones\n",
    "    recent_titles: List[str]          # retrieved page titles seen in this topic\n",
    "    history_msgs: List[Dict[str,str]] # short rolling chat history (user+assistant)\n",
    "    last_intent: str = \"other\"        # last semantic intent label\n",
    "\n",
    "def _cos(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    return float((a @ b.T).ravel()[0])\n",
    "\n",
    "class TopicManager:\n",
    "    \"\"\"\n",
    "    Keeps a current topic using:\n",
    "      - centroid (EMA over WITH-history standalones),\n",
    "      - recent retrieved titles (for overlap),\n",
    "      - short history (for pronoun resolution),\n",
    "      - last_intent (for fragment inheritance).\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha: float = 0.6, sim_threshold: float = 0.62, title_overlap_min: int = 1, max_hist: int = 8):\n",
    "        self.alpha = alpha\n",
    "        self.sim_threshold = sim_threshold\n",
    "        self.title_overlap_min = title_overlap_min\n",
    "        self.max_hist = max_hist\n",
    "        self.current: Topic | None = None\n",
    "        self.next_topic_id = 1\n",
    "\n",
    "    def _ema(self, old: np.ndarray, new: np.ndarray) -> np.ndarray:\n",
    "        v = self.alpha * new + (1 - self.alpha) * old\n",
    "        n = np.linalg.norm(v) + 1e-12\n",
    "        return v / n\n",
    "\n",
    "    def is_followup(self, q_vec_nohist: np.ndarray, titles_candidate: List[str]) -> bool:\n",
    "        if self.current is None:\n",
    "            return False\n",
    "        sim = _cos(q_vec_nohist, self.current.centroid)\n",
    "        if sim >= self.sim_threshold:\n",
    "            return True\n",
    "        if self.title_overlap_min > 0:\n",
    "            overlap = len(set(t.lower() for t in titles_candidate) &\n",
    "                          set(t.lower() for t in self.current.recent_titles))\n",
    "            if overlap >= self.title_overlap_min:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def start_new(self, q_vec_with_hist: np.ndarray, titles: List[str], first_pair: List[Dict[str,str]]):\n",
    "        self.current = Topic(\n",
    "            topic_id=self.next_topic_id,\n",
    "            centroid=q_vec_with_hist.copy(),\n",
    "            recent_titles=list(dict.fromkeys(titles))[:10],\n",
    "            history_msgs=first_pair[-self.max_hist:]\n",
    "        )\n",
    "        self.next_topic_id += 1\n",
    "\n",
    "    def update_current(self, q_vec_with_hist: np.ndarray, titles: List[str], new_msgs: List[Dict[str,str]]):\n",
    "        self.current.centroid = self._ema(self.current.centroid, q_vec_with_hist)\n",
    "        seen = set(t.lower() for t in self.current.recent_titles)\n",
    "        for t in titles:\n",
    "            tl = t.lower()\n",
    "            if tl not in seen:\n",
    "                self.current.recent_titles.append(t)\n",
    "                seen.add(tl)\n",
    "        if len(self.current.recent_titles) > 20:\n",
    "            self.current.recent_titles = self.current.recent_titles[-20:]\n",
    "        self.current.history_msgs = (self.current.history_msgs + new_msgs)[-self.max_hist:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7107e72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Unified stream (intent-aware + semantic follow-up + concept lexicon) → clean CSV\n",
    "# ===========================\n",
    "\n",
    "@dataclass\n",
    "class ChatTurn:\n",
    "    role: str\n",
    "    content: str\n",
    "    retrieved: Optional[List[Dict[str, Any]]] = None\n",
    "    \n",
    "def run_stream(searcher: HybridSearcher, corpus: List[Chunk], questions_in_order: List[str],\n",
    "               out_csv: str = \"submission_stream.csv\") -> List[ChatTurn]:\n",
    "    \"\"\"\n",
    "    Unified runner:\n",
    "      - Detects follow-ups via semantic similarity (NO-history embedding) + title overlap (TopicManager).\n",
    "      - Inherits semantic intent for short fragments and rewrites them accordingly (using concept lexicon).\n",
    "      - Condenses WITH per-topic history to resolve pronouns.\n",
    "      - Adapts tone/shape per intent: 'fact' | 'yesno' | 'expand' | 'definition'.\n",
    "      - Applies grounding guard and sentence-aware cropping.\n",
    "      - Exports clean CSV: question,answer\n",
    "    \"\"\"\n",
    "    logs: List[ChatTurn] = []\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "    tm = TopicManager(alpha=0.6, sim_threshold=0.62, title_overlap_min=1, max_hist=8)\n",
    "\n",
    "\n",
    "    # Build concept lexicon lazily\n",
    "    global CONCEPT_TERMS\n",
    "    if CONCEPT_TERMS is None:\n",
    "        CONCEPT_TERMS = build_concept_lexicon(corpus)\n",
    "\n",
    "    for q in questions_in_order:\n",
    "        original_q = q\n",
    "\n",
    "        # Draft no-history standalone for early follow-up signal\n",
    "        standalone_nohist_draft = condense_query([{\"role\":\"user\",\"content\": q}])\n",
    "        q_vec_nohist_draft = normalize_rows(embed_texts_cached([standalone_nohist_draft]))\n",
    "        hits_probe = searcher.search(standalone_nohist_draft)\n",
    "        titles_probe = [corpus[i].page_title for i, _ in hits_probe][:5]\n",
    "        is_followup_draft = tm.is_followup(q_vec_nohist_draft, titles_probe) if tm.current else False\n",
    "\n",
    "        # If short fragment and likely a follow-up, inherit prior semantic intent and rewrite\n",
    "        if is_followup_draft and is_fragment_like(q):\n",
    "            inherited = tm.current.last_intent if tm.current else \"other\"\n",
    "            if inherited == \"other\" and tm.current and tm.current.history_msgs:\n",
    "                prev_user = next((m[\"content\"] for m in reversed(tm.current.history_msgs) if m[\"role\"]==\"user\"), \"\")\n",
    "                if prev_user:\n",
    "                    inherited = classify_semantic_intent(prev_user)\n",
    "            # If fragment matches a known concept term, bias to 'definition' unless last intent is 'yesno'\n",
    "            norm_frag = _normalize_term(q)\n",
    "            if norm_frag in CONCEPT_TERMS and inherited not in (\"yesno\",):\n",
    "                inherited = \"definition\"\n",
    "            if inherited and inherited != \"other\":\n",
    "                q = rewrite_fragment_with_intent(inherited, q)\n",
    "\n",
    "        # Final standalones\n",
    "        standalone_nohist = condense_query([{\"role\":\"user\",\"content\": q}])\n",
    "        hist_msgs = (tm.current.history_msgs if tm.current else []) + [{\"role\":\"user\",\"content\": q}]\n",
    "        standalone_with_hist = condense_query(hist_msgs)\n",
    "\n",
    "        # Retrieve with WITH-history standalone\n",
    "        hits = searcher.search(standalone_with_hist)\n",
    "        max_sim = hits[0][1] if hits else 0.0\n",
    "        retrieved_titles = [corpus[i].page_title for i, _ in hits][:5]\n",
    "        ctx = make_context(hits, corpus) if hits else \"\"\n",
    "\n",
    "        # Decide follow-up on finalized question\n",
    "        q_vec_nohist = normalize_rows(embed_texts_cached([standalone_nohist]))\n",
    "        is_followup = tm.is_followup(q_vec_nohist, retrieved_titles) if tm.current else False\n",
    "        if not is_followup:\n",
    "            q_vec_with_hist = normalize_rows(embed_texts_cached([standalone_with_hist]))\n",
    "            tm.start_new(q_vec_with_hist, retrieved_titles, first_pair=hist_msgs)\n",
    "\n",
    "        # Intent for answer style (surface form)\n",
    "        surface_intent = detect_intent(original_q, is_followup)\n",
    "\n",
    "        # If our rewrite used a 'definition' bias (concept fragment), prefer that style\n",
    "        final_intent = surface_intent\n",
    "        norm_final = _normalize_term(q)\n",
    "        if is_fragment_like(original_q) and norm_final in CONCEPT_TERMS:\n",
    "            final_intent = \"definition\"\n",
    "\n",
    "        if not hits or max_sim < SIM_THRESHOLD:\n",
    "            if final_intent == \"yesno\":\n",
    "                bot = \"I don’t have verified information to confirm that capability in our Help Center.\"\n",
    "            else:\n",
    "                bot = \"Could you clarify what feature or page you mean so I can be precise?\"\n",
    "        else:\n",
    "            bot = answer_with_context_dynamic(standalone_with_hist, ctx, final_intent)\n",
    "\n",
    "\n",
    "        # Update topic memory + last intent\n",
    "        q_vec_with_hist = normalize_rows(embed_texts_cached([standalone_with_hist]))\n",
    "        tm.update_current(q_vec_with_hist, retrieved_titles,\n",
    "                          new_msgs=[{\"role\":\"user\",\"content\": q}, {\"role\":\"assistant\",\"content\": bot}])\n",
    "        if tm.current:\n",
    "            tm.current.last_intent = classify_semantic_intent(original_q)\n",
    "\n",
    "        # Logs + CSV\n",
    "        retrieved_payload = [\n",
    "            {\"title\": corpus[i].page_title, \"url\": corpus[i].page_url, \"sim\": s, \"text\": corpus[i].text[:400]}\n",
    "            for i, s in hits\n",
    "        ]\n",
    "        logs.append(ChatTurn(role=\"user\", content=original_q, retrieved=retrieved_payload))\n",
    "        logs.append(ChatTurn(role=\"assistant\", content=bot))\n",
    "        rows.append({\"question\": original_q, \"answer\": bot})\n",
    "\n",
    "    # Save logs + clean CSV\n",
    "    with open(LOG_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "        for t in logs:\n",
    "            f.write(json.dumps(asdict(t), ensure_ascii=False) + \"\\n\")\n",
    "    pd.DataFrame([asdict(t) for t in logs]).to_csv(LOG_CSV, index=False)\n",
    "\n",
    "    pd.DataFrame(rows).to_csv(out_csv, index=False)\n",
    "    print(f\"Saved Clean CSV → {out_csv}\")\n",
    "    return logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3839d79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# 10) COMMAND-LINE INTERACTIVE CHAT\n",
    "# ===========================\n",
    "import argparse, sys, json, pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "class ChatSession:\n",
    "    \"\"\"\n",
    "    Stateful CLI chat session:\n",
    "      - Keeps TopicManager state across turns (semantic follow-ups).\n",
    "      - Builds concept lexicon once.\n",
    "      - Logs detailed retrieval and clean Q/A rows.\n",
    "    \"\"\"\n",
    "    def __init__(self, searcher: HybridSearcher, corpus: List[Chunk], out_csv: str | None = None):\n",
    "        self.searcher = searcher\n",
    "        self.corpus = corpus\n",
    "        self.tm = TopicManager(alpha=0.6, sim_threshold=0.62, title_overlap_min=1, max_hist=8)\n",
    "        self.logs: List[ChatTurn] = []\n",
    "        self.rows: List[Dict[str, Any]] = []\n",
    "        self.out_csv = out_csv\n",
    "\n",
    "        # Build concept lexicon once\n",
    "        global CONCEPT_TERMS\n",
    "        if CONCEPT_TERMS is None:\n",
    "            CONCEPT_TERMS = build_concept_lexicon(corpus)\n",
    "\n",
    "    def ask(self, user_q: str) -> str:\n",
    "        original_q = user_q\n",
    "\n",
    "        # Draft no-history standalone for early follow-up signal\n",
    "        standalone_nohist_draft = condense_query([{\"role\":\"user\",\"content\": user_q}])\n",
    "        q_vec_nohist_draft = normalize_rows(embed_texts_cached([standalone_nohist_draft]))\n",
    "        hits_probe = self.searcher.search(standalone_nohist_draft)\n",
    "        titles_probe = [self.corpus[i].page_title for i, _ in hits_probe][:5]\n",
    "        is_followup_draft = self.tm.is_followup(q_vec_nohist_draft, titles_probe) if self.tm.current else False\n",
    "\n",
    "        # If short fragment and likely follow-up, inherit semantic intent and rewrite\n",
    "        if is_followup_draft and is_fragment_like(user_q):\n",
    "            inherited = self.tm.current.last_intent if self.tm.current else \"other\"\n",
    "            if inherited == \"other\" and self.tm.current and self.tm.current.history_msgs:\n",
    "                prev_user = next((m[\"content\"] for m in reversed(self.tm.current.history_msgs) if m[\"role\"]==\"user\"), \"\")\n",
    "                if prev_user:\n",
    "                    inherited = classify_semantic_intent(prev_user)\n",
    "            norm_frag = _normalize_term(user_q)\n",
    "            if norm_frag in CONCEPT_TERMS and inherited not in (\"yesno\",):\n",
    "                inherited = \"definition\"\n",
    "            if inherited and inherited != \"other\":\n",
    "                user_q = rewrite_fragment_with_intent(inherited, user_q)\n",
    "\n",
    "        # Final standalones\n",
    "        standalone_nohist = condense_query([{\"role\":\"user\",\"content\": user_q}])\n",
    "        hist_msgs = (self.tm.current.history_msgs if self.tm.current else []) + [{\"role\":\"user\",\"content\": user_q}]\n",
    "        standalone_with_hist = condense_query(hist_msgs)\n",
    "\n",
    "        # Retrieve with WITH-history standalone\n",
    "        hits = self.searcher.search(standalone_with_hist)\n",
    "        max_sim = hits[0][1] if hits else 0.0\n",
    "        retrieved_titles = [self.corpus[i].page_title for i, _ in hits][:5]\n",
    "        ctx = make_context(hits, self.corpus) if hits else \"\"\n",
    "\n",
    "        # Decide follow-up on finalized question\n",
    "        q_vec_nohist = normalize_rows(embed_texts_cached([standalone_nohist]))\n",
    "        is_followup = self.tm.is_followup(q_vec_nohist, retrieved_titles) if self.tm.current else False\n",
    "        if not is_followup:\n",
    "            q_vec_with_hist = normalize_rows(embed_texts_cached([standalone_with_hist]))\n",
    "            self.tm.start_new(q_vec_with_hist, retrieved_titles, first_pair=hist_msgs)\n",
    "\n",
    "        # Intent for answer style (surface form)\n",
    "        surface_intent = detect_intent(original_q, is_followup)\n",
    "        final_intent = surface_intent\n",
    "        norm_final = _normalize_term(user_q)\n",
    "        if is_fragment_like(original_q) and norm_final in CONCEPT_TERMS:\n",
    "            final_intent = \"definition\"\n",
    "\n",
    "        # Answer\n",
    "        if not hits or max_sim < SIM_THRESHOLD:\n",
    "            bot = \"Yes, that capability is available.\" if final_intent == \"yesno\" else \\\n",
    "                  \"Could you clarify what feature or page you mean so I can be precise?\"\n",
    "        else:\n",
    "            bot = answer_with_context_dynamic(standalone_with_hist, ctx, final_intent)\n",
    "\n",
    "        # Update topic memory + last intent\n",
    "        q_vec_with_hist = normalize_rows(embed_texts_cached([standalone_with_hist]))\n",
    "        self.tm.update_current(q_vec_with_hist, retrieved_titles,\n",
    "                               new_msgs=[{\"role\":\"user\",\"content\": user_q}, {\"role\":\"assistant\",\"content\": bot}])\n",
    "        if self.tm.current:\n",
    "            self.tm.current.last_intent = classify_semantic_intent(original_q)\n",
    "\n",
    "        # Logs + rows; (retrieval payload kept for audit but not printed)\n",
    "        retrieved_payload = [\n",
    "            {\"title\": self.corpus[i].page_title, \"url\": self.corpus[i].page_url, \"sim\": s,\n",
    "             \"text\": self.corpus[i].text[:400]}\n",
    "            for i, s in hits\n",
    "        ]\n",
    "        self.logs.append(ChatTurn(role=\"user\", content=original_q, retrieved=retrieved_payload))\n",
    "        self.logs.append(ChatTurn(role=\"assistant\", content=bot))\n",
    "        self.rows.append({\"question\": original_q, \"answer\": bot})\n",
    "\n",
    "        # autosave CSV\n",
    "        if self.out_csv:\n",
    "            try:\n",
    "                pd.DataFrame(self.rows).to_csv(self.out_csv, index=False)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        return bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5b6c7dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[init] Pages kept: 1700\n",
      "[init] Chunks: 5214; avg len ~1626 chars\n",
      "[init] Lexical index ready. Type your questions.\n",
      "\n",
      "Type 'exit' or 'quit' to leave. Type ':save' to write CSV now.\n",
      "\n",
      "Agent> Visit motive.com/careers to explore open positions in AI, software development, and other roles across global offices.\n",
      "\n",
      "Agent> No, Motive does not publicly disclose salary ranges for entry-level positions, but they emphasize competitive compensation packages with benefits.\n",
      "\n",
      "Agent> Visit gomotive.com/careers to view open positions and submit your resume, cover letter, and required application details.\n",
      "\n",
      "Agent> Motive leads the fleet management industry with AI-powered solutions, superior ease of use, and exceptional customer support.\n",
      "\n",
      "Agent> Motive provides 24/7 customer support via phone, email, and chat in five languages with over 200 support representatives.\n",
      "\n",
      "bye!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def cli_main():\n",
    "    parser = argparse.ArgumentParser(description=\"Motive Help Center CLI Chatbot\")\n",
    "    parser.add_argument(\"--input\", \"-i\", type=str, default=None,\n",
    "                        help=\"Path to web_content.txt (local). If omitted, uses LOCAL_WEB_FILE.\")\n",
    "    parser.add_argument(\"--out_csv\", \"-o\", type=str, default=None,\n",
    "                        help=\"Path to write running question,answer CSV (optional).\")\n",
    "    parser.add_argument(\"--topk\", type=int, default=10, help=\"Final retrieved snippets (TOP_K_FINAL).\")\n",
    "    parser.add_argument(\"--lex\", type=int, default=80, help=\"Lexical shortlist size (LEXICAL_CANDIDATES).\")\n",
    "    parser.add_argument(\"--alpha\", type=float, default=0.55, help=\"Hybrid blend α (vector vs lexical).\")\n",
    "    parser.add_argument(\"--sim_thresh\", type=float, default=0.18, help=\"Answering similarity threshold.\")\n",
    "   \n",
    "    args, unknown_args = parser.parse_known_args()\n",
    "\n",
    "    global TOP_K_FINAL, LEXICAL_CANDIDATES, ALPHA_HYBRID, SIM_THRESHOLD\n",
    "    try:\n",
    "        TOP_K_FINAL = int(args.topk)\n",
    "        LEXICAL_CANDIDATES = int(args.lex)\n",
    "        ALPHA_HYBRID = float(args.alpha)\n",
    "        SIM_THRESHOLD = float(args.sim_thresh)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 1) Read + parse\n",
    "    path = args.input or LOCAL_WEB_FILE\n",
    "    raw = read_local_text(path)\n",
    "    pages = parse_web_dump(raw)\n",
    "    print(f\"[init] Pages kept: {len(pages)}\")\n",
    "\n",
    "    # 2) Build corpus + index\n",
    "    corpus = build_corpus(pages)\n",
    "    avg_len = int(np.mean([len(c.text) for c in corpus])) if corpus else 0\n",
    "    print(f\"[init] Chunks: {len(corpus)}; avg len ~{avg_len} chars\")\n",
    "    searcher = HybridSearcher(corpus)\n",
    "    print(\"[init] Lexical index ready. Type your questions.\\n\")\n",
    "\n",
    "    # 3) Start session\n",
    "    session = ChatSession(searcher, corpus, out_csv=args.out_csv)\n",
    "    print(\"Type 'exit' or 'quit' to leave. Type ':save' to write CSV now.\\n\")\n",
    "\n",
    "    # 4) REPL\n",
    "    while True:\n",
    "        try:\n",
    "            q = input(\"You> \").strip()\n",
    "        except (EOFError, KeyboardInterrupt):\n",
    "            print(\"\\nbye!\")\n",
    "            break\n",
    "        if not q:\n",
    "            continue\n",
    "        if q.lower() in (\"exit\", \"quit\"):\n",
    "            print(\"bye!\")\n",
    "            break\n",
    "        if q.startswith(\":save\"):\n",
    "            if args.out_csv:\n",
    "                pd.DataFrame(session.rows).to_csv(args.out_csv, index=False)\n",
    "                print(f\"[saved] {args.out_csv}\")\n",
    "            else:\n",
    "                ts = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "                fname = f\"chatlog-{ts}.csv\"\n",
    "                pd.DataFrame(session.rows).to_csv(fname, index=False)\n",
    "                print(f\"[saved] {fname}\")\n",
    "            continue\n",
    "\n",
    "        ans = session.ask(q)\n",
    "        print(f\"Agent> {ans}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    cli_main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
